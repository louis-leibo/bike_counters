{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from skrub import TableVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import holidays\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the files\n",
    "df_train = pd.read_parquet(\"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/data/train.parquet\")\n",
    "df_test = pd.read_parquet(\"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/data/final_test.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add external data : weather data\n",
    "weather = pd.read_csv(\n",
    "    \"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/external_data/weather_data.csv.gz\",\n",
    "    parse_dates=[\"AAAAMMJJHH\"],\n",
    "    date_format=\"%Y%m%d%H\",\n",
    "    compression=\"gzip\",\n",
    "    sep=\";\",\n",
    ").rename(columns={\"AAAAMMJJHH\": \"date\"})\n",
    "\n",
    "weather = weather[\n",
    "    (weather[\"date\"] >= df_train[\"date\"].min() - datetime.timedelta(hours=1))\n",
    "    & (weather[\"date\"] <= df_test[\"date\"].max() + datetime.timedelta(hours=1))\n",
    "]\n",
    "\n",
    "weather_reduced = (\n",
    "    weather.drop(columns=[\"NUM_POSTE\", \"NOM_USUEL\", \"LAT\", \"LON\", \"QDXI3S\"])\n",
    "    .groupby(\"date\")\n",
    "    .mean()\n",
    "    .dropna(axis=1, how=\"all\")\n",
    "    .interpolate(method=\"linear\")\n",
    ")\n",
    "\n",
    "# Perform standardization, PCA, and create a DataFrame in one pipeline\n",
    "n_components = 11  # Retain ~90% of variance\n",
    "pca_columns = [f'pca_feature_{i+1}' for i in range(n_components)]  # Create PCA column names\n",
    "\n",
    "weather_pca_df = pd.DataFrame(\n",
    "    PCA(n_components=n_components).fit_transform(\n",
    "        StandardScaler().fit_transform(weather_reduced)\n",
    "    ),\n",
    "    columns=pca_columns,\n",
    "    index=weather_reduced.index  # Retain original index\n",
    ")\n",
    "\n",
    "# We merge only the TEMPERATURE feature\n",
    "df_train = df_train.merge(weather_pca_df, left_on=\"date\", right_on=\"date\", how=\"left\")\n",
    "df_test = df_test.merge(weather_pca_df, left_on=\"date\", right_on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the date feature on different time scales :\n",
    "fr_holidays = holidays.France()\n",
    "\n",
    "def _encode_dates(X):\n",
    "    X = X.copy()  # modify a copy of X\n",
    "    # Encode the date information from the DateOfDeparture columns\n",
    "    X[\"year\"] = X[\"date\"].dt.year\n",
    "    X[\"month\"] = X[\"date\"].dt.month\n",
    "    X[\"day\"] = X[\"date\"].dt.day\n",
    "    X[\"weekday\"] = X[\"date\"].dt.weekday\n",
    "    X[\"hour\"] = X[\"date\"].dt.hour\n",
    "\n",
    "    # creation of a binary varible depicting if day in weekend\n",
    "    X[\"is_weekend\"] = np.where(X[\"weekday\"] + 1 > 5, 1, 0)\n",
    "\n",
    "    # Add a feature to indicate if the day is a holiday in France\n",
    "    X[\"is_holiday\"] = X[\"date\"].apply(lambda d: 1 if d in fr_holidays else 0)\n",
    "\n",
    "    # Finally we can drop the original columns from the dataframe\n",
    "    return X.drop(columns=[\"date\"])\n",
    "\n",
    "df_train = _encode_dates(df_train)\n",
    "df_test = _encode_dates(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing :\n",
    "# Extract features from counter_installation_date\n",
    "for df in [df_train, df_test]:\n",
    "    df[\"installation_year\"] = df[\"counter_installation_date\"].dt.year\n",
    "    df[\"installation_month\"] = df[\"counter_installation_date\"].dt.month\n",
    "\n",
    "df_train = df_train.drop(columns=[\"counter_installation_date\"])\n",
    "df_test = df_test.drop(columns=[\"counter_installation_date\"])\n",
    "\n",
    "# Label encode high-cardinality categorical features\n",
    "label_encoders = {}\n",
    "for col in [\"counter_id\", \"site_id\", \"counter_name\", \"site_name\", \"counter_technical_id\", \"coordinates\"]:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col])\n",
    "    df_test[col] = le.fit_transform(df_test[col])\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Define bins and labels for temperature categories in Kelvin\\nbins = [-float('inf'), 278.15, 283, 298, 308.15, float('inf')]  # Updated Kelvin thresholds\\nlabels = ['very_cold', 'cold', 'moderate', 'warm', 'very_hot']\\n\\n# Create a new categorical feature for temperature\\ntraining_set_merged['temp_category'] = pd.cut(training_set_merged['temperature'], bins=bins, labels=labels)\\ntesting_set_merged['temp_category'] = pd.cut(testing_set_merged['temperature'], bins=bins, labels=labels)\\n\\n# One-hot encode the categories for the model\\ntraining_set_merged = pd.get_dummies(training_set_merged, columns=['temp_category'], drop_first=True)\\ntesting_set_merged = pd.get_dummies(testing_set_merged, columns=['temp_category'], drop_first=True)\\n\\n# remove temperature column :\\ntraining_set_merged = training_set_merged.drop(columns=['temperature'])\\ntesting_set_merged = testing_set_merged.drop(columns=['temperature'])\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Define bins and labels for temperature categories in Kelvin\n",
    "bins = [-float('inf'), 278.15, 283, 298, 308.15, float('inf')]  # Updated Kelvin thresholds\n",
    "labels = ['very_cold', 'cold', 'moderate', 'warm', 'very_hot']\n",
    "\n",
    "# Create a new categorical feature for temperature\n",
    "training_set_merged['temp_category'] = pd.cut(training_set_merged['temperature'], bins=bins, labels=labels)\n",
    "testing_set_merged['temp_category'] = pd.cut(testing_set_merged['temperature'], bins=bins, labels=labels)\n",
    "\n",
    "# One-hot encode the categories for the model\n",
    "training_set_merged = pd.get_dummies(training_set_merged, columns=['temp_category'], drop_first=True)\n",
    "testing_set_merged = pd.get_dummies(testing_set_merged, columns=['temp_category'], drop_first=True)\n",
    "\n",
    "# remove temperature column :\n",
    "training_set_merged = training_set_merged.drop(columns=['temperature'])\n",
    "testing_set_merged = testing_set_merged.drop(columns=['temperature'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns=[\"bike_count\", \"log_bike_count\"])\n",
    "y_train = df_train[\"log_bike_count\"]\n",
    "\n",
    "X_test = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 6, 9, 12],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 6, 9, 12],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'subsample': 0.7, 'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.1, 'colsample_bytree': 0.6}\n",
      "Best Score: 0.8679456370183681\n"
     ]
    }
   ],
   "source": [
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,  # Number of parameter settings sampled\n",
    "    scoring='neg_mean_squared_error',  # Use appropriate scoring metric\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available processors\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", -random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the model with the best parameters\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# Initialize the XGBoost regressor\n",
    "model = best_xgb_model\n",
    "\n",
    "# Fit the model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    ")\n",
    "\n",
    "# Make Predictions on Test Data\n",
    "y_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize the XGBoost regressor\\nmodel = XGBRegressor(\\n    # objective=\"reg:squarederror\",  # Use squared error for regression\\n    max_depth=6,                  # Maximum depth of the trees\\n    learning_rate=0.1,            # Step size shrinkage\\n    n_estimators=500,             # Number of boosting rounds\\n    subsample=0.8,                # Fraction of samples for training each tree\\n    colsample_bytree=0.8,         # Fraction of features for each tree\\n    random_state=42,              # Reproducibility\\n)\\n\\n# Fit the model\\nmodel.fit(\\n    X_train, y_train,\\n)\\n\\n# Make Predictions on Test Data\\ny_predictions = model.predict(X_test)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Initialize the XGBoost regressor\n",
    "model = XGBRegressor(\n",
    "    # objective=\"reg:squarederror\",  # Use squared error for regression\n",
    "    max_depth=6,                  # Maximum depth of the trees\n",
    "    learning_rate=0.1,            # Step size shrinkage\n",
    "    n_estimators=500,             # Number of boosting rounds\n",
    "    subsample=0.8,                # Fraction of samples for training each tree\n",
    "    colsample_bytree=0.8,         # Fraction of features for each tree\n",
    "    random_state=42,              # Reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    ")\n",
    "\n",
    "# Make Predictions on Test Data\n",
    "y_predictions = model.predict(X_test)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2882888 1.371596  2.1574965 ... 4.9524865 4.6113596 3.711801 ]\n"
     ]
    }
   ],
   "source": [
    "print(y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_predictions, columns=[\"log_bike_count\"]).reset_index().rename(\n",
    "    columns={\"index\": \"Id\"}\n",
    ").to_csv(\"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/predictions_XGboost_PCA_weather.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 0.3437612628878113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We can print the RMSE on the training data :\n",
    "y_train_predictions = model.predict(X_train)\n",
    "rmse_train = mean_squared_error(y_train, y_train_predictions, squared=False)\n",
    "print(f\"Training RMSE: {rmse_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/Dev_by_sam_&_louis/Model XGBoost simple avec weather PCA.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/louisleibovici/Documents/VS_Code/Bike_counters%20DSB%20Project/bike_counters/Dev_by_sam_%26_louis/Model%20XGBoost%20simple%20avec%20weather%20PCA.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# code to get feature importance :\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/louisleibovici/Documents/VS_Code/Bike_counters%20DSB%20Project/bike_counters/Dev_by_sam_%26_louis/Model%20XGBoost%20simple%20avec%20weather%20PCA.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/louisleibovici/Documents/VS_Code/Bike_counters%20DSB%20Project/bike_counters/Dev_by_sam_%26_louis/Model%20XGBoost%20simple%20avec%20weather%20PCA.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/louisleibovici/Documents/VS_Code/Bike_counters%20DSB%20Project/bike_counters/Dev_by_sam_%26_louis/Model%20XGBoost%20simple%20avec%20weather%20PCA.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Step 1: Extract the preprocessor and feature names\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/louisleibovici/Documents/VS_Code/Bike_counters%20DSB%20Project/bike_counters/Dev_by_sam_%26_louis/Model%20XGBoost%20simple%20avec%20weather%20PCA.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Retrieve the preprocessor from the pipeline\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/louisleibovici/Documents/VS_Code/Bike_counters%20DSB%20Project/bike_counters/Dev_by_sam_%26_louis/Model%20XGBoost%20simple%20avec%20weather%20PCA.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m preprocessor \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mnamed_steps[\u001b[39m'\u001b[39m\u001b[39mpreprocessor\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/louisleibovici/Documents/VS_Code/Bike_counters%20DSB%20Project/bike_counters/Dev_by_sam_%26_louis/Model%20XGBoost%20simple%20avec%20weather%20PCA.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Get the feature names after preprocessing\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/louisleibovici/Documents/VS_Code/Bike_counters%20DSB%20Project/bike_counters/Dev_by_sam_%26_louis/Model%20XGBoost%20simple%20avec%20weather%20PCA.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m feature_names \u001b[39m=\u001b[39m preprocessor\u001b[39m.\u001b[39mget_feature_names_out()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# code to get feature importance :\n",
    "\n",
    "\n",
    "# Step 1: Extract the preprocessor and feature names\n",
    "# Retrieve the preprocessor from the pipeline\n",
    "preprocessor = pipeline.named_steps['preprocessor']\n",
    "\n",
    "# Get the feature names after preprocessing\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Step 2: Extract the trained XGBoost model and feature importance\n",
    "xgb_model = pipeline.named_steps['model']\n",
    "\n",
    "# Get feature importances from the trained XGBoost model\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "\n",
    "# Step 3: Combine feature names and importance scores into a DataFrame\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "# Sort features by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(\"Top Features by Importance:\")\n",
    "importance_df.head(40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
