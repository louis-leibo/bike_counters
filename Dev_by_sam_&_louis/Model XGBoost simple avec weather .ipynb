{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from skrub import TableVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import holidays\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the files\n",
    "df_train = pd.read_parquet(\"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/data/train.parquet\")\n",
    "df_test = pd.read_parquet(\"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/data/final_test.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add external data : weather data\n",
    "weather = pd.read_csv(\n",
    "    \"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/external_data/weather_data.csv.gz\",\n",
    "    parse_dates=[\"AAAAMMJJHH\"],\n",
    "    date_format=\"%Y%m%d%H\",\n",
    "    compression=\"gzip\",\n",
    "    sep=\";\",\n",
    ").rename(columns={\"AAAAMMJJHH\": \"date\"})\n",
    "\n",
    "weather = weather[\n",
    "    (weather[\"date\"] >= df_train[\"date\"].min() - datetime.timedelta(hours=1))\n",
    "    & (weather[\"date\"] <= df_test[\"date\"].max() + datetime.timedelta(hours=1))\n",
    "]\n",
    "\n",
    "weather_reduced = (\n",
    "    weather.drop(columns=[\"NUM_POSTE\", \"NOM_USUEL\", \"LAT\", \"LON\", \"QDXI3S\"])\n",
    "    .groupby(\"date\")\n",
    "    .mean()\n",
    "    .dropna(axis=1, how=\"all\")\n",
    "    .interpolate(method=\"linear\")\n",
    ")\n",
    "\n",
    "# We merge only the TEMPERATURE feature\n",
    "df_train = df_train.merge(weather_reduced[\"T\"], left_on=\"date\", right_on=\"date\", how=\"left\")\n",
    "df_test = df_test.merge(weather_reduced[\"T\"], left_on=\"date\", right_on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the date feature on different time scales :\n",
    "fr_holidays = holidays.France()\n",
    "\n",
    "def _encode_dates(X):\n",
    "    X = X.copy()  # modify a copy of X\n",
    "    # Encode the date information from the DateOfDeparture columns\n",
    "    X[\"year\"] = X[\"date\"].dt.year\n",
    "    X[\"month\"] = X[\"date\"].dt.month\n",
    "    X[\"day\"] = X[\"date\"].dt.day\n",
    "    X[\"weekday\"] = X[\"date\"].dt.weekday\n",
    "    X[\"hour\"] = X[\"date\"].dt.hour\n",
    "\n",
    "    # creation of a binary varible depicting if day in weekend\n",
    "    X[\"is_weekend\"] = np.where(X[\"weekday\"] + 1 > 5, 1, 0)\n",
    "\n",
    "    # Add a feature to indicate if the day is a holiday in France\n",
    "    X[\"is_holiday\"] = X[\"date\"].apply(lambda d: 1 if d in fr_holidays else 0)\n",
    "\n",
    "    # Finally we can drop the original columns from the dataframe\n",
    "    return X.drop(columns=[\"date\"])\n",
    "\n",
    "df_train = _encode_dates(df_train)\n",
    "df_test = _encode_dates(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing :\n",
    "\n",
    "# Drop unnecessary columns :\n",
    "df_train = df_train.drop(columns=[\"coordinates\", \"counter_name\", \"site_name\", \"counter_technical_id\"])\n",
    "df_test = df_test.drop(columns=[\"coordinates\", \"counter_name\", \"site_name\", \"counter_technical_id\"])\n",
    "\n",
    "# Extract features from counter_installation_date\n",
    "df_train[\"installation_year\"] = df_train[\"counter_installation_date\"].dt.year\n",
    "df_train = df_train.drop(columns=[\"counter_installation_date\"])\n",
    "\n",
    "df_test[\"installation_year\"] = df_test[\"counter_installation_date\"].dt.year\n",
    "df_test = df_test.drop(columns=[\"counter_installation_date\"])\n",
    "\n",
    "# Label encode high-cardinality categorical features\n",
    "label_encoders = {}\n",
    "for col in [\"counter_id\", \"site_id\"]:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col])\n",
    "    df_test[col] = le.fit_transform(df_test[col])\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter_id</th>\n",
       "      <th>site_id</th>\n",
       "      <th>bike_count</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>log_bike_count</th>\n",
       "      <th>T</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>installation_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.250000</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>12.133333</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.616667</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>19.666667</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   counter_id  site_id  bike_count   latitude  longitude  log_bike_count  \\\n",
       "0           1        0         0.0  48.846028   2.375429        0.000000   \n",
       "1           1        0         1.0  48.846028   2.375429        0.693147   \n",
       "2           1        0         0.0  48.846028   2.375429        0.000000   \n",
       "3           1        0         4.0  48.846028   2.375429        1.609438   \n",
       "4           1        0         9.0  48.846028   2.375429        2.302585   \n",
       "\n",
       "           T  year  month  day  weekday  hour  is_weekend  is_holiday  \\\n",
       "0  12.250000  2020      9    1        1     2           0           0   \n",
       "1  12.133333  2020      9    1        1     3           0           0   \n",
       "2  11.616667  2020      9    1        1     4           0           0   \n",
       "3  19.666667  2020      9    1        1    15           0           0   \n",
       "4  18.750000  2020      9    1        1    18           0           0   \n",
       "\n",
       "   installation_year  \n",
       "0               2013  \n",
       "1               2013  \n",
       "2               2013  \n",
       "3               2013  \n",
       "4               2013  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Define bins and labels for temperature categories in Kelvin\\nbins = [-float('inf'), 278.15, 283, 298, 308.15, float('inf')]  # Updated Kelvin thresholds\\nlabels = ['very_cold', 'cold', 'moderate', 'warm', 'very_hot']\\n\\n# Create a new categorical feature for temperature\\ntraining_set_merged['temp_category'] = pd.cut(training_set_merged['temperature'], bins=bins, labels=labels)\\ntesting_set_merged['temp_category'] = pd.cut(testing_set_merged['temperature'], bins=bins, labels=labels)\\n\\n# One-hot encode the categories for the model\\ntraining_set_merged = pd.get_dummies(training_set_merged, columns=['temp_category'], drop_first=True)\\ntesting_set_merged = pd.get_dummies(testing_set_merged, columns=['temp_category'], drop_first=True)\\n\\n# remove temperature column :\\ntraining_set_merged = training_set_merged.drop(columns=['temperature'])\\ntesting_set_merged = testing_set_merged.drop(columns=['temperature'])\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Define bins and labels for temperature categories in Kelvin\n",
    "bins = [-float('inf'), 278.15, 283, 298, 308.15, float('inf')]  # Updated Kelvin thresholds\n",
    "labels = ['very_cold', 'cold', 'moderate', 'warm', 'very_hot']\n",
    "\n",
    "# Create a new categorical feature for temperature\n",
    "training_set_merged['temp_category'] = pd.cut(training_set_merged['temperature'], bins=bins, labels=labels)\n",
    "testing_set_merged['temp_category'] = pd.cut(testing_set_merged['temperature'], bins=bins, labels=labels)\n",
    "\n",
    "# One-hot encode the categories for the model\n",
    "training_set_merged = pd.get_dummies(training_set_merged, columns=['temp_category'], drop_first=True)\n",
    "testing_set_merged = pd.get_dummies(testing_set_merged, columns=['temp_category'], drop_first=True)\n",
    "\n",
    "# remove temperature column :\n",
    "training_set_merged = training_set_merged.drop(columns=['temperature'])\n",
    "testing_set_merged = testing_set_merged.drop(columns=['temperature'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns=[\"bike_count\", \"log_bike_count\"])\n",
    "y_train = df_train[\"log_bike_count\"]\n",
    "\n",
    "X_test = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost regressor\n",
    "model = XGBRegressor(\n",
    "    # objective=\"reg:squarederror\",  # Use squared error for regression\n",
    "    max_depth=6,                  # Maximum depth of the trees\n",
    "    learning_rate=0.1,            # Step size shrinkage\n",
    "    n_estimators=500,             # Number of boosting rounds\n",
    "    subsample=0.8,                # Fraction of samples for training each tree\n",
    "    colsample_bytree=0.8,         # Fraction of features for each tree\n",
    "    random_state=42,              # Reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    ")\n",
    "\n",
    "# Make Predictions on Test Data\n",
    "y_predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4304142 1.3584911 1.931674  ... 5.22543   4.7523494 3.8777204]\n"
     ]
    }
   ],
   "source": [
    "print(y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_predictions, columns=[\"log_bike_count\"]).reset_index().rename(\n",
    "    columns={\"index\": \"Id\"}\n",
    ").to_csv(\"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/predictions_XGboost_vsimple_weather_newdata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 0.39359653346822177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We can print the RMSE on the training data :\n",
    "y_train_predictions = model.predict(X_train)\n",
    "rmse_train = mean_squared_error(y_train, y_train_predictions, squared=False)\n",
    "print(f\"Training RMSE: {rmse_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to get feature importance :\n",
    "\n",
    "\n",
    "# Step 1: Extract the preprocessor and feature names\n",
    "# Retrieve the preprocessor from the pipeline\n",
    "preprocessor = pipeline.named_steps['preprocessor']\n",
    "\n",
    "# Get the feature names after preprocessing\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Step 2: Extract the trained XGBoost model and feature importance\n",
    "xgb_model = pipeline.named_steps['model']\n",
    "\n",
    "# Get feature importances from the trained XGBoost model\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "\n",
    "# Step 3: Combine feature names and importance scores into a DataFrame\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "# Sort features by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(\"Top Features by Importance:\")\n",
    "importance_df.head(40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
