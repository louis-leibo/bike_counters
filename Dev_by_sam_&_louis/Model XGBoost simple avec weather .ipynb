{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from skrub import TableVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import holidays\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the files\n",
    "df_train = pd.read_parquet(\"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/data/train.parquet\")\n",
    "df_test = pd.read_parquet(\"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/data/final_test.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add external data : weather data\n",
    "weather = pd.read_csv(\n",
    "    \"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/external_data/weather_data.csv.gz\",\n",
    "    parse_dates=[\"AAAAMMJJHH\"],\n",
    "    date_format=\"%Y%m%d%H\",\n",
    "    compression=\"gzip\",\n",
    "    sep=\";\",\n",
    ").rename(columns={\"AAAAMMJJHH\": \"date\"})\n",
    "\n",
    "weather = weather[\n",
    "    (weather[\"date\"] >= df_train[\"date\"].min() - datetime.timedelta(hours=1))\n",
    "    & (weather[\"date\"] <= df_test[\"date\"].max() + datetime.timedelta(hours=1))\n",
    "]\n",
    "\n",
    "weather_reduced = (\n",
    "    weather.drop(columns=[\"NUM_POSTE\", \"NOM_USUEL\", \"LAT\", \"LON\", \"QDXI3S\"])\n",
    "    .groupby(\"date\")\n",
    "    .mean()\n",
    "    .dropna(axis=1, how=\"all\")\n",
    "    .interpolate(method=\"linear\")\n",
    ")\n",
    "\n",
    "# We merge only the TEMPERATURE feature\n",
    "df_train = df_train.merge(weather_reduced[\"T\"], left_on=\"date\", right_on=\"date\", how=\"left\")\n",
    "df_test = df_test.merge(weather_reduced[\"T\"], left_on=\"date\", right_on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the date feature on different time scales :\n",
    "fr_holidays = holidays.France()\n",
    "\n",
    "def _encode_dates(X):\n",
    "    X = X.copy()  # modify a copy of X\n",
    "    # Encode the date information from the DateOfDeparture columns\n",
    "    X[\"year\"] = X[\"date\"].dt.year\n",
    "    X[\"month\"] = X[\"date\"].dt.month\n",
    "    X[\"day\"] = X[\"date\"].dt.day\n",
    "    X[\"weekday\"] = X[\"date\"].dt.weekday\n",
    "    X[\"hour\"] = X[\"date\"].dt.hour\n",
    "\n",
    "    # creation of a binary varible depicting if day in weekend\n",
    "    X[\"is_weekend\"] = np.where(X[\"weekday\"] + 1 > 5, 1, 0)\n",
    "\n",
    "    # Add a feature to indicate if the day is a holiday in France\n",
    "    X[\"is_holiday\"] = X[\"date\"].apply(lambda d: 1 if d in fr_holidays else 0)\n",
    "\n",
    "    # Finally we can drop the original columns from the dataframe\n",
    "    return X.drop(columns=[\"date\"])\n",
    "\n",
    "df_train = _encode_dates(df_train)\n",
    "df_test = _encode_dates(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing :\n",
    "\n",
    "# Extract features from counter_installation_date\n",
    "for df in [df_train, df_test]:\n",
    "    df[\"installation_year\"] = df[\"counter_installation_date\"].dt.year\n",
    "    df[\"installation_month\"] = df[\"counter_installation_date\"].dt.month\n",
    "\n",
    "df_train = df_train.drop(columns=[\"counter_installation_date\"])\n",
    "df_test = df_test.drop(columns=[\"counter_installation_date\"])\n",
    "\n",
    "# Label encode high-cardinality categorical features\n",
    "label_encoders = {}\n",
    "for col in [\"counter_id\", \"site_id\", \"counter_name\", \"site_name\", \"counter_technical_id\", \"coordinates\"]:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col])\n",
    "    df_test[col] = le.fit_transform(df_test[col])\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter_id</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>bike_count</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>counter_technical_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>log_bike_count</th>\n",
       "      <th>T</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>installation_year</th>\n",
       "      <th>installation_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.250000</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>12.133333</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.616667</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>19.666667</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   counter_id  counter_name  site_id  site_name  bike_count  coordinates  \\\n",
       "0           1            10        0          5         0.0           10   \n",
       "1           1            10        0          5         1.0           10   \n",
       "2           1            10        0          5         0.0           10   \n",
       "3           1            10        0          5         4.0           10   \n",
       "4           1            10        0          5         9.0           10   \n",
       "\n",
       "   counter_technical_id   latitude  longitude  log_bike_count          T  \\\n",
       "0                     0  48.846028   2.375429        0.000000  12.250000   \n",
       "1                     0  48.846028   2.375429        0.693147  12.133333   \n",
       "2                     0  48.846028   2.375429        0.000000  11.616667   \n",
       "3                     0  48.846028   2.375429        1.609438  19.666667   \n",
       "4                     0  48.846028   2.375429        2.302585  18.750000   \n",
       "\n",
       "   year  month  day  weekday  hour  is_weekend  is_holiday  installation_year  \\\n",
       "0  2020      9    1        1     2           0           0               2013   \n",
       "1  2020      9    1        1     3           0           0               2013   \n",
       "2  2020      9    1        1     4           0           0               2013   \n",
       "3  2020      9    1        1    15           0           0               2013   \n",
       "4  2020      9    1        1    18           0           0               2013   \n",
       "\n",
       "   installation_month  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   1  \n",
       "4                   1  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Define bins and labels for temperature categories in Kelvin\\nbins = [-float('inf'), 278.15, 283, 298, 308.15, float('inf')]  # Updated Kelvin thresholds\\nlabels = ['very_cold', 'cold', 'moderate', 'warm', 'very_hot']\\n\\n# Create a new categorical feature for temperature\\ntraining_set_merged['temp_category'] = pd.cut(training_set_merged['temperature'], bins=bins, labels=labels)\\ntesting_set_merged['temp_category'] = pd.cut(testing_set_merged['temperature'], bins=bins, labels=labels)\\n\\n# One-hot encode the categories for the model\\ntraining_set_merged = pd.get_dummies(training_set_merged, columns=['temp_category'], drop_first=True)\\ntesting_set_merged = pd.get_dummies(testing_set_merged, columns=['temp_category'], drop_first=True)\\n\\n# remove temperature column :\\ntraining_set_merged = training_set_merged.drop(columns=['temperature'])\\ntesting_set_merged = testing_set_merged.drop(columns=['temperature'])\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Define bins and labels for temperature categories in Kelvin\n",
    "bins = [-float('inf'), 278.15, 283, 298, 308.15, float('inf')]  # Updated Kelvin thresholds\n",
    "labels = ['very_cold', 'cold', 'moderate', 'warm', 'very_hot']\n",
    "\n",
    "# Create a new categorical feature for temperature\n",
    "training_set_merged['temp_category'] = pd.cut(training_set_merged['temperature'], bins=bins, labels=labels)\n",
    "testing_set_merged['temp_category'] = pd.cut(testing_set_merged['temperature'], bins=bins, labels=labels)\n",
    "\n",
    "# One-hot encode the categories for the model\n",
    "training_set_merged = pd.get_dummies(training_set_merged, columns=['temp_category'], drop_first=True)\n",
    "testing_set_merged = pd.get_dummies(testing_set_merged, columns=['temp_category'], drop_first=True)\n",
    "\n",
    "# remove temperature column :\n",
    "training_set_merged = training_set_merged.drop(columns=['temperature'])\n",
    "testing_set_merged = testing_set_merged.drop(columns=['temperature'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns=[\"bike_count\", \"log_bike_count\"])\n",
    "y_train = df_train[\"log_bike_count\"]\n",
    "\n",
    "X_test = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 6, 9, 12],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Parameters: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 9, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "Best Score: 0.8786247253791343\n"
     ]
    }
   ],
   "source": [
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", -random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the model with the best parameters\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# Initialize the XGBoost regressor\n",
    "model = best_xgb_model\n",
    "\n",
    "# Fit the model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    ")\n",
    "\n",
    "# Make Predictions on Test Data\n",
    "y_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize the XGBoost regressor\\nmodel = XGBRegressor(\\n    # objective=\"reg:squarederror\",  # Use squared error for regression\\n    max_depth=6,                  # Maximum depth of the trees\\n    learning_rate=0.1,            # Step size shrinkage\\n    n_estimators=500,             # Number of boosting rounds\\n    subsample=0.8,                # Fraction of samples for training each tree\\n    colsample_bytree=0.8,         # Fraction of features for each tree\\n    random_state=42,              # Reproducibility\\n)\\n\\n# Fit the model\\nmodel.fit(\\n    X_train, y_train,\\n)\\n\\n# Make Predictions on Test Data\\ny_predictions = model.predict(X_test)\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Initialize the XGBoost regressor\n",
    "model = XGBRegressor(\n",
    "    # objective=\"reg:squarederror\",  # Use squared error for regression\n",
    "    max_depth=6,                  # Maximum depth of the trees\n",
    "    learning_rate=0.1,            # Step size shrinkage\n",
    "    n_estimators=500,             # Number of boosting rounds\n",
    "    subsample=0.8,                # Fraction of samples for training each tree\n",
    "    colsample_bytree=0.8,         # Fraction of features for each tree\n",
    "    random_state=42,              # Reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    ")\n",
    "\n",
    "# Make Predictions on Test Data\n",
    "y_predictions = model.predict(X_test)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44584194 1.3020741  1.9596593  ... 5.2742405  4.8132505  4.154561  ]\n"
     ]
    }
   ],
   "source": [
    "print(y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_predictions, columns=[\"log_bike_count\"]).reset_index().rename(\n",
    "    columns={\"index\": \"Id\"}\n",
    ").to_csv(\"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/predictions_XGboost_vsimple_weather_newdata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 0.34111583962921876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We can print the RMSE on the training data :\n",
    "y_train_predictions = model.predict(X_train)\n",
    "rmse_train = mean_squared_error(y_train, y_train_predictions, squared=False)\n",
    "print(f\"Training RMSE: {rmse_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# code to get feature importance :\\n\\n\\n# Step 1: Extract the preprocessor and feature names\\n# Retrieve the preprocessor from the pipeline\\npreprocessor = pipeline.named_steps[\\'preprocessor\\']\\n\\n# Get the feature names after preprocessing\\nfeature_names = preprocessor.get_feature_names_out()\\n\\n# Step 2: Extract the trained XGBoost model and feature importance\\nxgb_model = pipeline.named_steps[\\'model\\']\\n\\n# Get feature importances from the trained XGBoost model\\nfeature_importance = xgb_model.feature_importances_\\n\\n# Step 3: Combine feature names and importance scores into a DataFrame\\nimportance_df = pd.DataFrame({\\'Feature\\': feature_names, \\'Importance\\': feature_importance})\\n\\n# Sort features by importance\\nimportance_df = importance_df.sort_values(by=\\'Importance\\', ascending=False)\\n\\n# Display top features\\nprint(\"Top Features by Importance:\")\\nimportance_df.head(40)\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# code to get feature importance :\n",
    "\n",
    "\n",
    "# Step 1: Extract the preprocessor and feature names\n",
    "# Retrieve the preprocessor from the pipeline\n",
    "preprocessor = pipeline.named_steps['preprocessor']\n",
    "\n",
    "# Get the feature names after preprocessing\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Step 2: Extract the trained XGBoost model and feature importance\n",
    "xgb_model = pipeline.named_steps['model']\n",
    "\n",
    "# Get feature importances from the trained XGBoost model\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "\n",
    "# Step 3: Combine feature names and importance scores into a DataFrame\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "# Sort features by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(\"Top Features by Importance:\")\n",
    "importance_df.head(40)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
