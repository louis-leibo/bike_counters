{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from skrub import TableVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import holidays\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the files\n",
    "df_train = pd.read_parquet(\"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/data/train.parquet\")\n",
    "df_test = pd.read_parquet(\"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/data/final_test.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add external data : weather data\n",
    "weather = pd.read_csv(\n",
    "    \"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/external_data/weather_data.csv.gz\",\n",
    "    parse_dates=[\"AAAAMMJJHH\"],\n",
    "    date_format=\"%Y%m%d%H\",\n",
    "    compression=\"gzip\",\n",
    "    sep=\";\",\n",
    ").rename(columns={\"AAAAMMJJHH\": \"date\"})\n",
    "\n",
    "weather = weather[\n",
    "    (weather[\"date\"] >= df_train[\"date\"].min() - datetime.timedelta(hours=1))\n",
    "    & (weather[\"date\"] <= df_test[\"date\"].max() + datetime.timedelta(hours=1))\n",
    "]\n",
    "\n",
    "weather_reduced = (\n",
    "    weather.drop(columns=[\"NUM_POSTE\", \"NOM_USUEL\", \"LAT\", \"LON\", \"QDXI3S\"])\n",
    "    .groupby(\"date\")\n",
    "    .mean()\n",
    "    .dropna(axis=1, how=\"all\")\n",
    "    .interpolate(method=\"linear\")\n",
    ")\n",
    "\n",
    "weather_reduced = (\n",
    "    weather_reduced\n",
    "    .drop(columns=[\n",
    "        \"PSTAT\", \"DD\", \"PMER\", \"PMERMIN\", \"QNEIGETOT\", \"QTCHAUSSEE\", \"ALTI\", \"QDRR1\", \"DXY\", \"FXY\",\n",
    "        \"QTNSOL\", \"QPMER\", \"DXI\", \"QFF\", \"QGLO2\", \"QGLO\", \"FF\", \"QHFXI3S\", \"QINS2\", \"QINS\",\n",
    "        \"QFXI3S\", \"RR1\", \"NEIGETOT\", 'HXI', 'HFXI3S', \"HTN\", \"HTX\", \"HUN\", \"HUX\", \"FXI3S\",\n",
    "        \"T10\", \"T20\", \"T50\", \"T100\", \"TNSOL\", \"TN50\", \"TCHAUSSEE\", \"TN\", \"TX\"\n",
    "    ])\n",
    "    .dropna(axis=1, how=\"all\")\n",
    "    .loc[:, weather_reduced.nunique(dropna=True) > 1]\n",
    "    .drop(columns=[\"QTD\", \"QTN\", \"QUN\", \"QUX\", \"QTSV\", \"QTX\", \"GLO2\", \"INS2\", \"UN\", \"UX\"])\n",
    ")\n",
    "\n",
    "# We merge :\n",
    "df_train = df_train.merge(weather_reduced, left_on=\"date\", right_on=\"date\", how=\"left\")\n",
    "df_test = df_test.merge(weather_reduced, left_on=\"date\", right_on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add jour ferie data\n",
    "jour_feries = (\n",
    "    pd.read_csv(\n",
    "        \"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/external_data/jours_feries_metropole.csv\",\n",
    "        date_format=\"%Y%m%d%H\"  # Ensure date format is handled correctly\n",
    "    )\n",
    "    .drop(columns=[\"annee\", \"zone\"])  # Drop unnecessary columns\n",
    ")\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "jour_feries['date'] = pd.to_datetime(jour_feries['date'])\n",
    "\n",
    "# Filter rows based on the date range of df_train and df_test\n",
    "jour_feries = jour_feries[\n",
    "    (jour_feries[\"date\"] >= df_train[\"date\"].min() - datetime.timedelta(hours=1))\n",
    "    & (jour_feries[\"date\"] <= df_test[\"date\"].max() + datetime.timedelta(hours=1))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mouvements sociaux data :\n",
    "mouvements_sociaux = (\n",
    "    pd.read_csv(\n",
    "        \"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/external_data/mouvements-sociaux-depuis-2002.csv\",\n",
    "        date_format=\"%Y%m%d%H\",\n",
    "        sep=\";\"\n",
    "    )\n",
    "    .drop(columns=['date_de_fin', 'Organisations syndicales', 'Métiers ciblés par le préavis',\n",
    "                   'Population devant travailler ciblee par le préavis', 'Nombre de grévistes du préavis'])  # Drop unnecessary columns\n",
    ")\n",
    "\n",
    "mouvements_sociaux['Date'] = pd.to_datetime(mouvements_sociaux['Date'])\n",
    "\n",
    "mouvements_sociaux = mouvements_sociaux[\n",
    "    (mouvements_sociaux[\"Date\"] >= df_train[\"date\"].min() - datetime.timedelta(hours=1))\n",
    "    & (mouvements_sociaux[\"Date\"] <= df_test[\"date\"].max() + datetime.timedelta(hours=1))\n",
    "]\n",
    "\n",
    "mouvements_sociaux = mouvements_sociaux[mouvements_sociaux['Date'] != pd.Timestamp('2021-03-08')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the date feature on different time scales :\n",
    "fr_holidays = holidays.France()\n",
    "\n",
    "def _encode_dates(X):\n",
    "    X = X.copy()  # modify a copy of X\n",
    "    # Encode the date information from the DateOfDeparture columns\n",
    "    X[\"year\"] = X[\"date\"].dt.year\n",
    "    X[\"month\"] = X[\"date\"].dt.month\n",
    "    X[\"day\"] = X[\"date\"].dt.day\n",
    "    X[\"weekday\"] = X[\"date\"].dt.weekday\n",
    "    X[\"hour\"] = X[\"date\"].dt.hour\n",
    "\n",
    "    # creation of a binary varible depicting if day in weekend\n",
    "    X[\"is_weekend\"] = np.where(X[\"weekday\"] + 1 > 5, 1, 0)\n",
    "\n",
    "    # Add a feature to indicate if the day is a holiday in France\n",
    "    X[\"is_holiday\"] = X[\"date\"].apply(lambda d: 1 if d in fr_holidays else 0)\n",
    "\n",
    "    # and if it is a jour ferie in France :\n",
    "    X[\"is_jour_ferie\"] = X[\"date\"].dt.date.isin(jour_feries['date']).astype(int)\n",
    "\n",
    "    # and it is a jour of \"mouvement social\" in France :\n",
    "    X[\"is_jour_mouvement_social\"] = X[\"date\"].dt.date.isin(mouvements_sociaux['Date']).astype(int)\n",
    "\n",
    "    # Finally we can drop the original columns from the dataframe\n",
    "    return X.drop(columns=[\"date\"])\n",
    "\n",
    "df_train = _encode_dates(df_train)\n",
    "df_test = _encode_dates(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing :\n",
    "\n",
    "# Extract features from counter_installation_date\n",
    "for df in [df_train, df_test]:\n",
    "    df[\"installation_year\"] = df[\"counter_installation_date\"].dt.year\n",
    "    df[\"installation_month\"] = df[\"counter_installation_date\"].dt.month\n",
    "\n",
    "df_train = df_train.drop(columns=[\"counter_installation_date\"])\n",
    "df_test = df_test.drop(columns=[\"counter_installation_date\"])\n",
    "\n",
    "# Label encode high-cardinality categorical features\n",
    "label_encoders = {}\n",
    "for col in [\"counter_id\", \"site_id\", \"counter_name\", \"site_name\", \"counter_technical_id\", \"coordinates\"]:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col])\n",
    "    df_test[col] = le.fit_transform(df_test[col])\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter_id</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>bike_count</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>counter_technical_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>log_bike_count</th>\n",
       "      <th>...</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_jour_ferie</th>\n",
       "      <th>is_jour_mouvement_social</th>\n",
       "      <th>installation_year</th>\n",
       "      <th>installation_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   counter_id  counter_name  site_id  site_name  bike_count  coordinates  \\\n",
       "0           1            10        0          5         0.0           10   \n",
       "1           1            10        0          5         1.0           10   \n",
       "2           1            10        0          5         0.0           10   \n",
       "3           1            10        0          5         4.0           10   \n",
       "4           1            10        0          5         9.0           10   \n",
       "\n",
       "   counter_technical_id   latitude  longitude  log_bike_count  ...  month  \\\n",
       "0                     0  48.846028   2.375429        0.000000  ...      9   \n",
       "1                     0  48.846028   2.375429        0.693147  ...      9   \n",
       "2                     0  48.846028   2.375429        0.000000  ...      9   \n",
       "3                     0  48.846028   2.375429        1.609438  ...      9   \n",
       "4                     0  48.846028   2.375429        2.302585  ...      9   \n",
       "\n",
       "   day  weekday  hour  is_weekend  is_holiday  is_jour_ferie  \\\n",
       "0    1        1     2           0           0              0   \n",
       "1    1        1     3           0           0              0   \n",
       "2    1        1     4           0           0              0   \n",
       "3    1        1    15           0           0              0   \n",
       "4    1        1    18           0           0              0   \n",
       "\n",
       "   is_jour_mouvement_social  installation_year  installation_month  \n",
       "0                         0               2013                   1  \n",
       "1                         0               2013                   1  \n",
       "2                         0               2013                   1  \n",
       "3                         0               2013                   1  \n",
       "4                         0               2013                   1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['counter_id', 'counter_name', 'site_id', 'site_name', 'bike_count',\n",
       "       'coordinates', 'counter_technical_id', 'latitude', 'longitude',\n",
       "       'log_bike_count', 'DRR1', 'HXY', 'FXI', 'T', 'QT', 'TD', 'DG', 'U',\n",
       "       'QU', 'DHUMI40', 'DHUMI80', 'TSV', 'VV', 'WW', 'GLO', 'INS', 'year',\n",
       "       'month', 'day', 'weekday', 'hour', 'is_weekend', 'is_holiday',\n",
       "       'is_jour_ferie', 'is_jour_mouvement_social', 'installation_year',\n",
       "       'installation_month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ArithmeticError# Creation of bins for the temperature feature : less than 0 degrees, between 0 and 5, between 5 and 15, between 15 and 30, more than 30\\nbins = [-float(\\'inf\\'), 0, 5, 15, 30, float(\\'inf\\')]\\nlabels = [\"< 0°C\", \"0-5°C\", \"5-15°C\", \"15-30°C\", \"> 30°C\"]\\n\\n# Create a new categorical feature for temperature\\ndf_train[\\'temp_category\\'] = pd.cut(df_train[\\'T\\'], bins=bins, labels=labels)\\ndf_test[\\'temp_category\\'] = pd.cut(df_test[\\'T\\'], bins=bins, labels=labels)\\n\\n# One-hot encode the categories for the model\\ndf_train = pd.get_dummies(df_train, columns=[\\'temp_category\\'], drop_first=True).astype(int)\\ndf_test = pd.get_dummies(df_test, columns=[\\'temp_category\\'], drop_first=True).astype(int)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''ArithmeticError# Creation of bins for the temperature feature : less than 0 degrees, between 0 and 5, between 5 and 15, between 15 and 30, more than 30\n",
    "bins = [-float('inf'), 0, 5, 15, 30, float('inf')]\n",
    "labels = [\"< 0°C\", \"0-5°C\", \"5-15°C\", \"15-30°C\", \"> 30°C\"]\n",
    "\n",
    "# Create a new categorical feature for temperature\n",
    "df_train['temp_category'] = pd.cut(df_train['T'], bins=bins, labels=labels)\n",
    "df_test['temp_category'] = pd.cut(df_test['T'], bins=bins, labels=labels)\n",
    "\n",
    "# One-hot encode the categories for the model\n",
    "df_train = pd.get_dummies(df_train, columns=['temp_category'], drop_first=True).astype(int)\n",
    "df_test = pd.get_dummies(df_test, columns=['temp_category'], drop_first=True).astype(int)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns=[\"bike_count\", \"log_bike_count\"])\n",
    "y_train = df_train[\"log_bike_count\"]\n",
    "\n",
    "X_test = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 6, 9, 12],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Parameters: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 9, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "Best Score: 0.8906274546492916\n"
     ]
    }
   ],
   "source": [
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", -random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the model with the best parameters\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# Initialize the XGBoost regressor\n",
    "model = best_xgb_model\n",
    "\n",
    "# Fit the model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    ")\n",
    "\n",
    "# Make Predictions on Test Data\n",
    "y_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize the XGBoost regressor\\nmodel = XGBRegressor(\\n    # objective=\"reg:squarederror\",  # Use squared error for regression\\n    max_depth=6,                  # Maximum depth of the trees\\n    learning_rate=0.1,            # Step size shrinkage\\n    n_estimators=500,             # Number of boosting rounds\\n    subsample=0.8,                # Fraction of samples for training each tree\\n    colsample_bytree=0.8,         # Fraction of features for each tree\\n    random_state=42,              # Reproducibility\\n)\\n\\n# Fit the model\\nmodel.fit(\\n    X_train, y_train,\\n)\\n\\n# Make Predictions on Test Data\\ny_predictions = model.predict(X_test)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Initialize the XGBoost regressor\n",
    "model = XGBRegressor(\n",
    "    # objective=\"reg:squarederror\",  # Use squared error for regression\n",
    "    max_depth=6,                  # Maximum depth of the trees\n",
    "    learning_rate=0.1,            # Step size shrinkage\n",
    "    n_estimators=500,             # Number of boosting rounds\n",
    "    subsample=0.8,                # Fraction of samples for training each tree\n",
    "    colsample_bytree=0.8,         # Fraction of features for each tree\n",
    "    random_state=42,              # Reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    ")\n",
    "\n",
    "# Make Predictions on Test Data\n",
    "y_predictions = model.predict(X_test)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.406196  1.4767338 2.0724719 ... 5.172175  4.7162175 3.6701746]\n"
     ]
    }
   ],
   "source": [
    "print(y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_predictions, columns=[\"log_bike_count\"]).reset_index().rename(\n",
    "    columns={\"index\": \"Id\"}\n",
    ").to_csv(\"/Users/louisleibovici/Documents/VS_Code/Bike_counters DSB Project/bike_counters/predictions_XGboost_vsimple_weather_newdata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 0.32663356896716444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We can print the RMSE on the training data :\n",
    "y_train_predictions = model.predict(X_train)\n",
    "rmse_train = mean_squared_error(y_train, y_train_predictions, squared=False)\n",
    "print(f\"Training RMSE: {rmse_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# code to get feature importance :\\n\\n\\n# Step 1: Extract the preprocessor and feature names\\n# Retrieve the preprocessor from the pipeline\\npreprocessor = pipeline.named_steps[\\'preprocessor\\']\\n\\n# Get the feature names after preprocessing\\nfeature_names = preprocessor.get_feature_names_out()\\n\\n# Step 2: Extract the trained XGBoost model and feature importance\\nxgb_model = pipeline.named_steps[\\'model\\']\\n\\n# Get feature importances from the trained XGBoost model\\nfeature_importance = xgb_model.feature_importances_\\n\\n# Step 3: Combine feature names and importance scores into a DataFrame\\nimportance_df = pd.DataFrame({\\'Feature\\': feature_names, \\'Importance\\': feature_importance})\\n\\n# Sort features by importance\\nimportance_df = importance_df.sort_values(by=\\'Importance\\', ascending=False)\\n\\n# Display top features\\nprint(\"Top Features by Importance:\")\\nimportance_df.head(40)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# code to get feature importance :\n",
    "\n",
    "\n",
    "# Step 1: Extract the preprocessor and feature names\n",
    "# Retrieve the preprocessor from the pipeline\n",
    "preprocessor = pipeline.named_steps['preprocessor']\n",
    "\n",
    "# Get the feature names after preprocessing\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Step 2: Extract the trained XGBoost model and feature importance\n",
    "xgb_model = pipeline.named_steps['model']\n",
    "\n",
    "# Get feature importances from the trained XGBoost model\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "\n",
    "# Step 3: Combine feature names and importance scores into a DataFrame\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "# Sort features by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(\"Top Features by Importance:\")\n",
    "importance_df.head(40)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
